{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmZ7LXl6-Q7E"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('popular')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYyJ0h1m-Zke",
        "outputId": "506a75fe-6642-4d2f-a15a-cd7205c118e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "!pip install flashtext\n",
        "!pip install pywsd\n",
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDSJfjjb-ysE",
        "outputId": "03617294-5422-4431-ef9e-0a6e74fbf836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-rjaras2_\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /tmp/pip-req-build-rjaras2_\n",
            "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.8.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.2.2)\n",
            "Collecting unidecode (from pke==2.0.0)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (0.18.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (1.4.0)\n",
            "Requirement already satisfied: spacy>=3.2.3 in /usr/local/lib/python3.10/dist-packages (from pke==2.0.0) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pke==2.0.0) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pke==2.0.0) (2023.12.25)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pke==2.0.0) (3.4.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.3->pke==2.0.0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=3.2.3->pke==2.0.0) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.1.5)\n",
            "Building wheels for collected packages: pke\n",
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6160628 sha256=edfc0a8a7221ab544528630d736f89ebcec9927e863d5ffa91cfbeb4ce8511a6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k2pfopdp/wheels/8c/07/29/6b35bed2aa36e33d77ff3677eb716965ece4d2e56639ad0aab\n",
            "Successfully built pke\n",
            "Installing collected packages: unidecode, pke\n",
            "Successfully installed pke-2.0.0 unidecode-1.3.8\n",
            "Collecting flashtext\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9296 sha256=976e75c884f1647ebaf7c1ae447a8f140850aa89e261dd641045ce1709361bab\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/be/39/c37ad168eb2ff644c9685f52554440372129450f0b8ed203dd\n",
            "Successfully built flashtext\n",
            "Installing collected packages: flashtext\n",
            "Successfully installed flashtext-2.7\n",
            "Collecting pywsd\n",
            "  Downloading pywsd-1.2.5-py3-none-any.whl (26.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from pywsd) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pywsd) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from pywsd) (2.0.3)\n",
            "Collecting wn==0.0.23 (from pywsd)\n",
            "  Downloading wn-0.0.23.tar.gz (31.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.6/31.6 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pywsd) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->pywsd) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->pywsd) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->pywsd) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->pywsd) (4.66.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->pywsd) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pywsd) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->pywsd) (2024.1)\n",
            "Building wheels for collected packages: wn\n",
            "  Building wheel for wn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wn: filename=wn-0.0.23-py3-none-any.whl size=31792911 sha256=914e99a35259acb24e402e69930732bcee323c5b8d06e64ab87fab416f720823\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/1a/7d/23a76ce45998af60e47466a694c237fa26023c5674b47672b2\n",
            "Successfully built wn\n",
            "Installing collected packages: wn, pywsd\n",
            "Successfully installed pywsd-1.2.5 wn-0.0.23\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "import pprint\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCNPf8meH4Uu",
        "outputId": "98d91962-3906-43b1-e8e3-675a4df8b5c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYWNha5zISx7",
        "outputId": "9a93b626-0e96-40eb-c3a5-2b48e9c5d670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sense2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6MMR7GgIXui",
        "outputId": "13eaa971-a37d-4366-a101-e63d43496152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sense2vec\n",
            "  Downloading sense2vec-2.0.2-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<4.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from sense2vec) (3.7.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from sense2vec) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from sense2vec) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from sense2vec) (2.0.10)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from sense2vec) (1.25.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (8.2.3)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (2.7.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.0.0->sense2vec) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->sense2vec) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->sense2vec) (2.18.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->sense2vec) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->sense2vec) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->sense2vec) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->sense2vec) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.0.0->sense2vec) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.0.0->sense2vec) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.0.0->sense2vec) (2.1.5)\n",
            "Installing collected packages: sense2vec\n",
            "Successfully installed sense2vec-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
        "!tar -xvf  s2v_reddit_2015_md.tar.gz\n",
        "\n",
        "# load sense2vec vectors\n",
        "from sense2vec import Sense2Vec\n",
        "s2v = Sense2Vec().from_disk('s2v_old')\n",
        "\n",
        "def edits(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz '+string.punctuation\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om0QvbPqHo9-",
        "outputId": "82495bfd-b8ce-4edb-9b55-e86b1bcf017d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-21 05:14:50--  https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240421%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240421T051450Z&X-Amz-Expires=300&X-Amz-Signature=2c0d5caed3b6a8e9384af2ba5c35460cf60b9bf44d478ade83ef7f6886ecf140&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=50261113&response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-04-21 05:14:50--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/50261113/52126080-0993-11ea-8190-8f0e295df22a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240421%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240421T051450Z&X-Amz-Expires=300&X-Amz-Signature=2c0d5caed3b6a8e9384af2ba5c35460cf60b9bf44d478ade83ef7f6886ecf140&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=50261113&response-content-disposition=attachment%3B%20filename%3Ds2v_reddit_2015_md.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600444501 (573M) [application/octet-stream]\n",
            "Saving to: ‘s2v_reddit_2015_md.tar.gz’\n",
            "\n",
            "s2v_reddit_2015_md. 100%[===================>] 572.63M   180MB/s    in 3.3s    \n",
            "\n",
            "2024-04-21 05:14:54 (172 MB/s) - ‘s2v_reddit_2015_md.tar.gz’ saved [600444501/600444501]\n",
            "\n",
            "./._s2v_old\n",
            "./s2v_old/\n",
            "./s2v_old/._freqs.json\n",
            "./s2v_old/freqs.json\n",
            "./s2v_old/._vectors\n",
            "./s2v_old/vectors\n",
            "./s2v_old/._cfg\n",
            "./s2v_old/cfg\n",
            "./s2v_old/._strings.json\n",
            "./s2v_old/strings.json\n",
            "./s2v_old/._key2row\n",
            "./s2v_old/key2row\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import requests\n",
        "import json\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import pke\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "import collections\n",
        "from collections import OrderedDict\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('famous')\n",
        "\n",
        "# Function to initialize the T5 question generation model and tokenizer\n",
        "def question_model_tokenizer():\n",
        "    question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "    question_tokenizer = T5Tokenizer.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "    return question_model, question_tokenizer\n",
        "\n",
        "# Function to generate a question using the T5 model\n",
        "def get_question(sentence, answer, mdl, tknizer):\n",
        "    text = \"context: {} answer: {}\".format(sentence, answer)\n",
        "    max_len = 256\n",
        "    encoding = tknizer.encode_plus(text, max_length=max_len, pad_to_max_length=False, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "    outs = mdl.generate(input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        early_stopping=True,\n",
        "                        num_beams=5,\n",
        "                        num_return_sequences=1,\n",
        "                        no_repeat_ngram_size=2,\n",
        "                        max_length=300)\n",
        "\n",
        "    dec = [tknizer.decode(ids, skip_special_tokens=True) for ids in outs]\n",
        "    question = dec[0].replace(\"question:\", \"\")\n",
        "    question = question.strip()\n",
        "    return question\n",
        "\n",
        "# Function to tokenize sentences from the text\n",
        "def tokenize_sentences(text):\n",
        "    sentences = [sent_tokenize(text)]\n",
        "    sentences = [y for x in sentences for y in x]\n",
        "    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n",
        "    return sentences\n",
        "\n",
        "# Function to extract keywords from the text\n",
        "def Keyword_Extraction(text):\n",
        "    extractor = pke.unsupervised.TfIdf()\n",
        "    extractor.load_document(input=text, language='en', normalization=None)\n",
        "    extractor.candidate_selection(n=3)\n",
        "    extractor.candidate_weighting()\n",
        "    keyphrases_TFIDF = extractor.get_n_best(n=10)\n",
        "\n",
        "    extractor = pke.unsupervised.KPMiner()\n",
        "    extractor.load_document(input=text, language='en', normalization=None)\n",
        "    extractor.candidate_selection(lasf=5, cutoff=200)\n",
        "    extractor.candidate_weighting(alpha=2.3, sigma=3.0)\n",
        "    keyphrases_KPMiner = extractor.get_n_best(n=10)\n",
        "\n",
        "    extractor = pke.unsupervised.YAKE()\n",
        "    extractor.load_document(input=text, language='en', normalization=None)\n",
        "    extractor.candidate_selection(n=3)\n",
        "    extractor.candidate_weighting(window=2, use_stems=False)\n",
        "    keyphrases_Yake = extractor.get_n_best(n=10, threshold=0.8)\n",
        "\n",
        "    pos = {'NOUN', 'PROPN','ADJ'}\n",
        "    extractor = pke.unsupervised.TextRank()\n",
        "    extractor.load_document(input=text, language='en', normalization=None)\n",
        "    extractor.candidate_weighting(window=2, pos=pos, top_percent=0.33)\n",
        "    keyphrases_TextRank = extractor.get_n_best(n=10)\n",
        "\n",
        "    extractor = pke.unsupervised.SingleRank()\n",
        "    extractor.load_document(input=text, language='en', normalization=None)\n",
        "    extractor.candidate_selection(pos=pos)\n",
        "    extractor.candidate_weighting(window=10, pos=pos)\n",
        "    keyphrases_SingleRank = extractor.get_n_best(n=10)\n",
        "\n",
        "    extractor = pke.unsupervised.TopicRank()\n",
        "    extractor.load_document(input=text)\n",
        "    extractor.candidate_selection(pos=pos)\n",
        "    extractor.candidate_weighting(threshold=0.74, method='average')\n",
        "    keyphrases_TopicRank = extractor.get_n_best(n=10)\n",
        "\n",
        "    grammar = \"NP: {<ADJ>*<NOUN|PROPN>+}\"\n",
        "    extractor = pke.unsupervised.TopicalPageRank()\n",
        "    extractor.load_document(input=text, language='en', normalization=None)\n",
        "    extractor.candidate_selection(grammar=grammar)\n",
        "    extractor.candidate_weighting(window=10, pos=pos)\n",
        "    keyphrases_TopicalPageRank = extractor.get_n_best(n=10)\n",
        "\n",
        "    extractor = pke.unsupervised.PositionRank()\n",
        "    extractor.load_document(input=text, language='en', normalization=None)\n",
        "    extractor.candidate_selection(grammar=grammar, maximum_word_number=3)\n",
        "    extractor.candidate_weighting(window=10, pos=pos)\n",
        "    keyphrases_PositionRank = extractor.get_n_best(n=10)\n",
        "\n",
        "    extractor = pke.unsupervised.MultipartiteRank()\n",
        "    extractor.load_document(input=text)\n",
        "    extractor.candidate_selection(pos=pos)\n",
        "    extractor.candidate_weighting(alpha=1.1, threshold=0.74, method='average')\n",
        "    keyphrases_MultipartiteRank = extractor.get_n_best(n=10)\n",
        "\n",
        "    stoplist = stopwords.words('english')\n",
        "    extractor = pke.supervised.Kea()\n",
        "    extractor.load_document(input=text, language='en', normalization=None)\n",
        "    extractor.candidate_selection()\n",
        "    extractor.candidate_weighting()\n",
        "    keyphrases_Kea = extractor.get_n_best(n=10)\n",
        "\n",
        "    extractor = pke.supervised.WINGNUS()\n",
        "    extractor.load_document(input=text)\n",
        "    extractor.candidate_selection()\n",
        "    extractor.candidate_weighting()\n",
        "    keyphrases_WINGNUS = extractor.get_n_best(n=10)\n",
        "\n",
        "    Keywords = keyphrases_TFIDF + keyphrases_KPMiner + keyphrases_Yake + keyphrases_TextRank + keyphrases_SingleRank + keyphrases_TopicRank + keyphrases_TopicalPageRank + keyphrases_PositionRank + keyphrases_MultipartiteRank + keyphrases_Kea + keyphrases_WINGNUS\n",
        "    Keywords = [k for k, v in Keywords]\n",
        "\n",
        "    duplicated_Keywords = [item for item, count in collections.Counter(Keywords).items() if count > 1]\n",
        "    return duplicated_Keywords\n",
        "\n",
        "# Function to get distractors using WordNet\n",
        "def get_distractors_wordnet(syn, word):\n",
        "    distractors = []\n",
        "    word = word.lower()\n",
        "    orig_word = word\n",
        "    if len(word.split()) > 0:\n",
        "        word = word.replace(\" \", \"_\")\n",
        "    hypernym = syn.hypernyms()\n",
        "    if len(hypernym) == 0:\n",
        "        return distractors\n",
        "    for item in hypernym[0].hyponyms():\n",
        "        name = item.lemmas()[0].name()\n",
        "        if name == orig_word:\n",
        "            continue\n",
        "        name = name.replace(\"_\", \" \")\n",
        "        name = \" \".join(w.capitalize() for w in name.split())\n",
        "        if name is not None and name not in distractors:\n",
        "            distractors.append(name)\n",
        "    return distractors\n",
        "\n",
        "# Function to get distractors using ConceptNet\n",
        "def get_distractors_conceptnet(word):\n",
        "    word = word.lower()\n",
        "    original_word = word\n",
        "    if len(word.split()) > 0:\n",
        "        word = word.replace(\" \", \"_\")\n",
        "    distractor_list = []\n",
        "    url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\" % (word, word)\n",
        "    obj = requests.get(url).json()\n",
        "\n",
        "    for edge in obj['edges']:\n",
        "        link = edge['end']['term']\n",
        "        url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\" % (link, link)\n",
        "        obj2 = requests.get(url2).json()\n",
        "        for edge in obj2['edges']:\n",
        "            word2 = edge['start']['label']\n",
        "            if word2 not in distractor_list and original_word.lower() not in word2.lower():\n",
        "                distractor_list.append(word2)\n",
        "\n",
        "    return distractor_list\n",
        "\n",
        "# Function to get distractors using sense2vec\n",
        "def get_distractors_sense2vec(word, s2v):\n",
        "    output = []\n",
        "    if word is None:\n",
        "        return output\n",
        "    word_preprocessed = word.translate(word.maketrans(\"\", \"\", string.punctuation))\n",
        "    word_preprocessed = word_preprocessed.lower()\n",
        "    word_edits = edits(word_preprocessed)\n",
        "    word = word.replace(\" \", \"_\")\n",
        "    sense = s2v.get_best_sense(word)\n",
        "    if sense is None:\n",
        "        return output\n",
        "    most_similar = s2v.most_similar(sense, n=15)\n",
        "    compare_list = [word_preprocessed]\n",
        "    for each_word in most_similar:\n",
        "        append_word = each_word[0].split(\"|\")[0].replace(\"_\", \" \")\n",
        "        append_word = append_word.strip()\n",
        "        append_word_processed = append_word.lower()\n",
        "        append_word_processed = append_word_processed.translate(append_word_processed.maketrans(\"\", \"\", string.punctuation))\n",
        "        if append_word_processed not in compare_list and word_preprocessed not in append_word_processed and append_word_processed not in word_edits:\n",
        "            output.append(append_word.title())\n",
        "            compare_list.append(append_word_processed)\n",
        "    out = list(OrderedDict.fromkeys(output))\n",
        "    return out\n",
        "\n",
        "# Function to generate MCQ options for each keyword\n",
        "def generate_mcq_options(keyword):\n",
        "    synsets = wn.synsets(keyword, 'n')\n",
        "    distractors_wordnet = []\n",
        "    for syn in synsets:\n",
        "        distractors_wordnet += get_distractors_wordnet(syn, keyword)\n",
        "    distractors_conceptnet = get_distractors_conceptnet(keyword)\n",
        "    distractors_sense2vec = get_distractors_sense2vec(keyword, s2v)\n",
        "    distractors = list(set(distractors_wordnet + distractors_conceptnet + distractors_sense2vec))\n",
        "    options = [keyword] + random.sample(distractors, min(3, len(distractors)))\n",
        "    random.shuffle(options)\n",
        "    return options\n",
        "\n",
        "# Function to get sentences containing keywords\n",
        "def get_sentences_for_keyword(keywords, sentences):\n",
        "    keyword_sentence_mapping = {}\n",
        "    for keyword in keywords:\n",
        "        keyword_sentence_mapping[keyword] = []\n",
        "        for sentence in sentences:\n",
        "            if keyword.lower() in sentence.lower():\n",
        "                keyword_sentence_mapping[keyword].append(sentence)\n",
        "    return keyword_sentence_mapping\n",
        "\n",
        "# Function to generate questions and MCQs from the text\n",
        "def generate_questions_and_mcqs(text):\n",
        "    mdl, tknizer = question_model_tokenizer()\n",
        "    sentences = tokenize_sentences(text)\n",
        "    duplicated_keywords = Keyword_Extraction(text)\n",
        "    keyword_sentence_mapping = get_sentences_for_keyword(duplicated_keywords, sentences)\n",
        "\n",
        "    question_number = 1\n",
        "    generated_questions = set()  # Keep track of generated questions to avoid duplicates\n",
        "    for keyword, sentences in keyword_sentence_mapping.items():\n",
        "        if keyword in generated_questions:  # Skip if question already generated for this keyword\n",
        "            continue\n",
        "        generated_questions.add(keyword)\n",
        "        sentence = random.choice(sentences)  # Randomly select one sentence for the keyword\n",
        "        answer = keyword\n",
        "        short_sentence = sentence[:200]  # Limiting the sentence to 200 characters\n",
        "        question = get_question(short_sentence, answer, mdl, tknizer)\n",
        "        options = generate_mcq_options(keyword)\n",
        "        if len(options) == 4:\n",
        "            print(f\"Q.{question_number}) {question}\")\n",
        "            for idx, option in enumerate(options, start=97):  # start=97 corresponds to 'a' in ASCII\n",
        "                print(f\"{chr(idx)}. {option}\")\n",
        "            print()\n",
        "            question_number += 1\n",
        "\n",
        "# Your provided text here\n",
        "text = \"\"\"\n",
        "Elon Musk, the enigmatic entrepreneur and visionary, stands as a modern-day icon of innovation and ambition. Born in South Africa in 1971, Musk's relentless pursuit of technological advancement has propelled him to the forefront of several industries. Co-founding Zip2, an online city guide software company, at the age of 24 marked his entry into the world of entrepreneurship. Later, he played pivotal roles in the creation of PayPal, SpaceX, Tesla, Neuralink, and The Boring Company, each revolutionizing its respective field. Musk's audacious vision extends beyond Earth, with SpaceX aiming to make humanity multi-planetary by colonizing Mars. His electric car company, Tesla, has redefined the automotive industry, pioneering sustainable transportation solutions. Musk's ventures into artificial intelligence, brain-computer interfaces, and underground transportation systems underscore his commitment to pushing the boundaries of technology. Despite facing criticism and setbacks, Musk's unwavering determination, coupled with his unorthodox approach, continues to inspire millions worldwide, earning him a place among the most influential figures of the 21st century.\n",
        "\"\"\"\n",
        "\n",
        "# Generate questions and MCQs from the text\n",
        "generate_questions_and_mcqs(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCIeTdzWCkRb",
        "outputId": "ff0a8d47-6156-464b-85c1-3937daf02de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Error loading famous: Package 'famous' not found in index\n",
            "WARNING:root:LoadFile._df_counts is hard coded to /usr/local/lib/python3.10/dist-packages/pke/models/df-semeval2010.tsv.gz\n",
            "WARNING:root:LoadFile._df_counts is hard coded to /usr/local/lib/python3.10/dist-packages/pke/models/df-semeval2010.tsv.gz\n",
            "WARNING:root:Candidates are generated using 0.33-top\n",
            "WARNING:root:LDA model is hard coded to /usr/local/lib/python3.10/dist-packages/pke/models/lda-1000-semeval2010.py3.pickle.gz\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  warnings.warn(\n",
            "WARNING:root:LoadFile._df_counts is hard coded to /usr/local/lib/python3.10/dist-packages/pke/models/df-semeval2010.tsv.gz\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MultinomialNB from version 0.20.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "WARNING:root:LoadFile._df_counts is hard coded to /usr/local/lib/python3.10/dist-packages/pke/models/df-semeval2010.tsv.gz\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MultinomialNB from version 0.20.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q.1) What is Elon Musk's nickname?\n",
            "a. Royal Jelly\n",
            "b. Perspiration\n",
            "c. musk\n",
            "d. Spacex\n",
            "\n",
            "Q.2) What was the name of the Boring Company?\n",
            "a. company\n",
            "b. Working Group\n",
            "c. Section\n",
            "d. Tammany Hall\n",
            "\n",
            "Q.3) Along with SpaceX, Neuralink, and The Boring Company, what was the name of a company created by Tesla?\n",
            "a. Gauss\n",
            "b. Electric Cars\n",
            "c. Car Company\n",
            "d. tesla\n",
            "\n",
            "Q.4) Along with artificial intelligence and brain-computer interfaces, what underground system did Musk explore?\n",
            "a. Anathematization\n",
            "b. Field\n",
            "c. Department Of Energy\n",
            "d. transportation\n",
            "\n",
            "Q.5) What is Musk's nickname?\n",
            "a. Space X\n",
            "b. Musk\n",
            "c. Richard Branson\n",
            "d. elon\n",
            "\n",
            "Q.6) Who is an enigmatic entrepreneur and visionary?\n",
            "a. Elon\n",
            "b. elon musk\n",
            "c. Jeff Bezos\n",
            "d. Space X\n",
            "\n",
            "Q.7) What type of entrepreneur is Elon Musk?\n",
            "a. enigmatic\n",
            "b. Comedic\n",
            "c. Profound\n",
            "d. Eccentric\n",
            "\n",
            "Q.8) What is Elon Musk's career?\n",
            "a. entrepreneur\n",
            "b. Successful Business\n",
            "c. Business Person\n",
            "d. Importer\n",
            "\n",
            "Q.9) Where was Musk born?\n",
            "a. Zimbabwe\n",
            "b. mozambique\n",
            "c. south africa\n",
            "d. Zambia\n",
            "\n",
            "Q.10) What has Musk been at the forefront of?\n",
            "a. Various Industries\n",
            "b. Many Industries\n",
            "c. several industries\n",
            "d. Multiple Industries\n",
            "\n",
            "Q.11) What was Musk's relentless pursuit of?\n",
            "a. Technological Progression\n",
            "b. Technological Innovation\n",
            "c. Technological Revolution\n",
            "d. technological advancement\n",
            "\n",
            "Q.12) What has pushed Musk to the forefront of several industries?\n",
            "a. Self-Destruction\n",
            "b. Noble Goal\n",
            "c. relentless pursuit\n",
            "d. Greater Goal\n",
            "\n",
            "Q.13) What type of company is Tesla?\n",
            "a. electric car company\n",
            "b. Musk\n",
            "c. Elon Musk\n",
            "d. Tesla Motors\n",
            "\n",
            "Q.14) Along with brain-computer interfaces and underground transportation systems, what technology did Musk explore?\n",
            "a. Artificial Consciousness\n",
            "b. artificial intelligence\n",
            "c. Advanced Ai\n",
            "d. Human Intelligence\n",
            "\n",
            "Q.15) What type of person is Elon Musk?\n",
            "a. visionary\n",
            "b. Virgo\n",
            "c. Tiger\n",
            "d. Heterosexual\n",
            "\n"
          ]
        }
      ]
    }
  ]
}